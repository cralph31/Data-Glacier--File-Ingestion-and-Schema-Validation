{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09a5b63a-6c95-4dd6-996f-646c859bb951",
   "metadata": {},
   "source": [
    "![title](data_glacier_screenshot_of_assignment.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4d46da-50f4-41fa-b21a-fc2e63644c47",
   "metadata": {},
   "source": [
    "## Week #6 Assignment- Data Ingestion & Schema- Carmelo R Casiraro- Data Glacier- Batch Code: LISUM34 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431d9f4a-cea1-47bb-a3e6-3c173dde9d49",
   "metadata": {},
   "source": [
    "# Step 1- Generate Training Data (Run Locally)\n",
    "### This routine will produce 40 million lines of output. This will most likely cause the kernel to crash if using an online Jupyter notebook editor. Therefore, this code should be downloaded onto your local system and run from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef751af-6995-4da6-a27e-a3ce1b1abc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "def generate_label(feature1, feature2, feature3):\n",
    "    \"\"\"\n",
    "    Generates a label based on the three features.\n",
    "    For simplicity, we'll use a basic rule-based approach.\n",
    "    \"\"\"\n",
    "    if feature1 + feature2 > feature3:\n",
    "        return 'A'\n",
    "    elif feature1 - feature2 < feature3:\n",
    "        return 'B'\n",
    "    else:\n",
    "        return 'C'\n",
    "\n",
    "def generate_data(num_samples):\n",
    "    \"\"\"\n",
    "    Generates synthetic data with three numerical features and a label.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for _ in range(num_samples):\n",
    "        feature1 = random.uniform(0, 100)\n",
    "        feature2 = random.uniform(0, 100)\n",
    "        feature3 = random.uniform(0, 100)\n",
    "        label = generate_label(feature1, feature2, feature3)\n",
    "        data.append([feature1, feature2, feature3, label])\n",
    "    return data\n",
    "\n",
    "def write_csv(file_name, data):\n",
    "    \"\"\"\n",
    "    Writes the generated data to a CSV file.\n",
    "    \"\"\"\n",
    "    header = ['Feature1', 'Feature2', 'Feature3', 'Label']\n",
    "    with open(file_name, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(header)\n",
    "        writer.writerows(data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_samples = 40000000\n",
    "    data = generate_data(num_samples)\n",
    "    write_csv('training_data.csv', data)\n",
    "    print(f\"Generated {num_samples} samples and written to 'training_data.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78437be2-05e5-4160-85e0-6b1c1574196a",
   "metadata": {},
   "source": [
    "### Step 1.1- After running the above code locally (NOT in this notebook), you can run the following code to verify that the output is indeed > 2GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309827b9-7058-4760-81a9-c49eaace199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def get_file_size(file_path):\n",
    "    try:\n",
    "        size = os.path.getsize(file_path)\n",
    "        return size\n",
    "    except OSError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "def human_readable_size(size, decimal_places=2):\n",
    "    for unit in ['B', 'KB', 'MB', 'GB', 'TB', 'PB']:\n",
    "        if size < 1024.0:\n",
    "            return f\"{size:.{decimal_places}f} {unit}\"\n",
    "        size /= 1024.0\n",
    "print(human_readable_size(get_file_size(\"test_data.csv\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a34d0ad-b60d-43c1-a208-39bc8d55c631",
   "metadata": {},
   "source": [
    "# Step 2- Upload file to google drive for hosting\n",
    "### Attempting to read the entire >2GB file into memory from this notebook will overload the kernel. Therefore, we need to stream the data from a third party. Here, we use google drive. Alternatives are available, but the code to interact with them may be slightly different.\n",
    "\n",
    "### Step 2.1- Go to Google Drive and + to Upload File\n",
    "![title](step2.1_googledrive_uploadfile.png)\n",
    "\n",
    "\n",
    "### Click on file where the data set is to upload file on your local system\n",
    "![title](step2.1_googledrive_clickondatasetfiletoupload.png)\n",
    "\n",
    "### Step 2.2- Add the collection of the file id once it is uploaded\n",
    "![title](getfileid_googledriveurllink_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a2975c-82fc-4af0-b203-617aa5b26116",
   "metadata": {},
   "source": [
    "# Step 3- Define function to get download link from Google Drive\n",
    "### if you type the link to the file into the address bar, you'll see a warning screen that this file is too big to scan for viruses. Google will download the file only once you submit the form. So, we use the requests library to get the form and then use beautifulsoup to parse and submit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "271e5adf-bdce-4398-90d7-c8c59b47daf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_download_link(file_id):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "    session = requests.Session()\n",
    "    response = session.get(URL, params={'id': file_id})\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    form = soup.find('form', {'id': 'download-form'})\n",
    "    if not form:\n",
    "        raise ValueError(\"Couldn't find the download form in the response.\")\n",
    "    \n",
    "    download_url = form['action']\n",
    "    params = {input['name']: input['value'] for input in form.find_all('input') if 'name' in input.attrs}\n",
    "    \n",
    "    return download_url, params\n",
    "\n",
    "file_id = \"1bW3a-Ym2WnUlZdqwzm5a9yVGSGLeLtrU\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abecfac-b46a-48ed-8c0f-5fe95e11f3e1",
   "metadata": {},
   "source": [
    "# Step 4 - Read with Pandas\n",
    "### To avoid overloading the kernel, we stream the file and read in chunks of 10000 rows at a time. You can adjust the chunk_size parameter as needed. The larger this value, the faster the routine will run, but the more memory it will consume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "435cb146-250a-4fc6-a564-e3ffc0007149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header: Feature1,Feature2,Feature3,Label\n",
      "Total rows processed: 40000000\n",
      "Time taken: 96.90 seconds\n",
      "Processing speed: 412797.86 rows/second\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def process_csv_with_pandas(file_id, chunk_size=10000):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    download_url, params = get_download_link(file_id)\n",
    "    \n",
    "    with requests.get(download_url, params=params, stream=True) as response:\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        lines = (line.decode('utf-8') for line in response.iter_lines())\n",
    "        \n",
    "        header = next(lines)\n",
    "        print(f\"Header: {header}\")\n",
    "        \n",
    "        csv_reader = csv.reader(lines)\n",
    "        \n",
    "        total_rows = 0\n",
    "        chunk = []\n",
    "        \n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            chunk.append(row)\n",
    "            if len(chunk) == chunk_size:\n",
    "                df = pd.DataFrame(chunk, columns=header.split(','))\n",
    "                total_rows += len(df)\n",
    "                chunk = []\n",
    "        \n",
    "        if chunk:\n",
    "            df = pd.DataFrame(chunk, columns=header.split(','))\n",
    "            total_rows += len(df)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        \n",
    "        print(f\"Total rows processed: {total_rows}\")\n",
    "        print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "        print(f\"Processing speed: {total_rows / elapsed_time:.2f} rows/second\")\n",
    "        \n",
    "process_csv_with_pandas(file_id, chunk_size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3747aea-db9c-4763-abee-dbcf64de7204",
   "metadata": {},
   "source": [
    "# Step 5- Read with Dask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1b2b35c-2a13-44ab-8cc5-a5c467c981d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header: Feature1,Feature2,Feature3,Label\n",
      "Total rows processed: 40000000\n",
      "Time taken: 116.24 seconds\n",
      "Processing speed: 344117.44 rows/second\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import csv\n",
    "import time\n",
    "import dask.dataframe as dd\n",
    "import requests\n",
    "\n",
    "def process_csv_with_dask(file_id, chunk_size=10000):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    download_url, params = get_download_link(file_id)\n",
    "    \n",
    "    with requests.get(download_url, params=params, stream=True) as response:\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        lines = (line.decode('utf-8') for line in response.iter_lines())\n",
    "        \n",
    "        header = next(lines)\n",
    "        print(f\"Header: {header}\")\n",
    "        \n",
    "        csv_reader = csv.reader(lines)\n",
    "        \n",
    "        total_rows = 0\n",
    "        chunk = []\n",
    "        \n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            chunk.append(row)\n",
    "            if len(chunk) == chunk_size:\n",
    "                df = dd.from_pandas(pd.DataFrame(chunk, columns=header.split(',')), npartitions=1)\n",
    "                total_rows += len(df.compute())\n",
    "                chunk = []\n",
    "        \n",
    "        if chunk:\n",
    "            df = dd.from_pandas(pd.DataFrame(chunk, columns=header.split(',')), npartitions=1)\n",
    "            total_rows += len(df.compute())\n",
    "        \n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        \n",
    "        print(f\"Total rows processed: {total_rows}\")\n",
    "        print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "        print(f\"Processing speed: {total_rows / elapsed_time:.2f} rows/second\")\n",
    "        \n",
    "process_csv_with_dask(file_id, chunk_size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2700e1-2179-491a-8829-46d0a75c131a",
   "metadata": {},
   "source": [
    "## Step 6- Read with Modin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146d225e-57f3-4f8d-b4ad-e6ed2b6ee606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header: Feature1,Feature2,Feature3,Label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: The size of /dev/shm is too small (67084288 bytes). The required size at least half of RAM (16582985728 bytes). Please, delete files in /dev/shm or increase size of /dev/shm with --shm-size in Docker. Also, you can can override the memory size for each Ray worker (in bytes) to the MODIN_MEMORY environment variable.\n",
      "2024-07-11 01:50:22,819\tWARNING services.py:2009 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67084288 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=2.58gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2024-07-11 01:50:24,118\tINFO worker.py:1771 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import csv\n",
    "import time\n",
    "import modin.pandas as pd\n",
    "import requests\n",
    "\n",
    "# Initialize Modin to use Ray as the execution engine\n",
    "import os\n",
    "os.environ[\"MODIN_ENGINE\"] = \"ray\"  # or \"dask\" if you prefer to use Dask as the engine\n",
    "\n",
    "def process_csv_with_modin(file_id, chunk_size=10000):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    download_url, params = get_download_link(file_id)\n",
    "    \n",
    "    with requests.get(download_url, params=params, stream=True) as response:\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        lines = (line.decode('utf-8') for line in response.iter_lines())\n",
    "        \n",
    "        header = next(lines)\n",
    "        print(f\"Header: {header}\")\n",
    "        \n",
    "        csv_reader = csv.reader(lines)\n",
    "        \n",
    "        total_rows = 0\n",
    "        chunk = []\n",
    "        \n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            chunk.append(row)\n",
    "            if len(chunk) == chunk_size:\n",
    "                df = pd.DataFrame(chunk, columns=header.split(','))\n",
    "                total_rows += len(df)\n",
    "                chunk = []\n",
    "        \n",
    "        if chunk:\n",
    "            df = pd.DataFrame(chunk, columns=header.split(','))\n",
    "            total_rows += len(df)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        \n",
    "        print(f\"Total rows processed: {total_rows}\")\n",
    "        print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "        print(f\"Processing speed: {total_rows / elapsed_time:.2f} rows/second\")\n",
    "        \n",
    "process_csv_with_modin(file_id, chunk_size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace73ddd-cb38-4c07-837c-46d2e9e0ecaa",
   "metadata": {},
   "source": [
    "## Step 7- Create a YAML File & Validate Data Set with YAML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ffb8bf4-d7de-424e-893c-94a95b6f4f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting schema.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile schema.yaml\n",
    "columns:\n",
    "- name: Feature1\n",
    "  type: float\n",
    "- name: Feature2\n",
    "  type: float\n",
    "- name: Feature3\n",
    "  type: float\n",
    "- name: Label\n",
    "  type: string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eeef15-fb90-49eb-a35a-eb8e0a71782e",
   "metadata": {},
   "source": [
    "## Step 8- Use the YAML file to validate the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f94ae314-8099-4770-9f3f-b31055a79d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data validation completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import requests\n",
    "import csv\n",
    "import io\n",
    "\n",
    "def validate_data(file_id, schema_file):\n",
    "    with open(schema_file, 'r') as file:\n",
    "        schema = yaml.safe_load(file)\n",
    "    \n",
    "    expected_columns = [col['name'] for col in schema['columns']]\n",
    "    \n",
    "    download_url, params = get_download_link(file_id)\n",
    "    \n",
    "    with requests.get(download_url, params=params, stream=True) as response:\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Read the first line of the file to get the header\n",
    "        header_line = next(response.iter_lines()).decode('utf-8')\n",
    "        header = header_line.split(',')\n",
    "        \n",
    "        # Check if the number of columns matches the expected count\n",
    "        if len(header) != len(expected_columns):\n",
    "            raise ValueError(f\"Incorrect number of columns. Expected: {len(expected_columns)}, Got: {len(header)}\")\n",
    "        \n",
    "        # Check if the column names match the expected names\n",
    "        if header != expected_columns:\n",
    "            raise ValueError(f\"Column names do not match schema. Expected: {expected_columns}, Got: {header}\")\n",
    "    \n",
    "    print(\"Data validation completed successfully.\")\n",
    "\n",
    "validate_data(file_id, 'schema.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45bf48a-1492-4009-b013-173c0af0ca6e",
   "metadata": {},
   "source": [
    "## Step 9- Output in gz format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853f6d8e-8a55-470a-b907-20935056f9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "def convert_to_pipe_separated_gz(file_id, output_file, chunk_size=10000):\n",
    "    download_url, params = get_download_link(file_id)\n",
    "    \n",
    "    with requests.get(download_url, params=params, stream=True) as response, gzip.open(output_file, 'wt') as gz_file:\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        lines = (line.decode('utf-8') for line in response.iter_lines())\n",
    "        \n",
    "        header = next(lines).split(',')\n",
    "        gz_file.write('|'.join(header) + '\\n')\n",
    "        \n",
    "        csv_reader = csv.reader(lines)\n",
    "        \n",
    "        for row in csv_reader:\n",
    "            if row:\n",
    "                gz_file.write('|'.join(row) + '\\n')\n",
    "    \n",
    "    print(f\"File converted and saved as: {output_file}\")\n",
    "\n",
    "convert_to_pipe_separated_gz(file_id, 'output_data.txt.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d76863-54bb-4e30-81b8-95c2c87429aa",
   "metadata": {},
   "source": [
    "## Step 10- Create a Summary report of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3923fac9-f332-41a2-9987-1994642aa3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def print_file_summary(file_path):\n",
    "    with gzip.open(file_path, 'rt') as file:\n",
    "        header = next(file).strip().split('|')\n",
    "        num_columns = len(header)\n",
    "        \n",
    "        num_rows = sum(1 for line in file) + 1  \n",
    "        # Add 1 to include the header\n",
    "    \n",
    "    file_size = os.path.getsize(file_path)\n",
    "    \n",
    "    print(f\"File Summary for {file_path}:\")\n",
    "    print(f\"Number of rows: {num_rows}\")\n",
    "    print(f\"Number of columns: {num_columns}\")\n",
    "    print(f\"File size: {file_size} bytes\")\n",
    "\n",
    "print_file_summary('output_data.txt.gz')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
